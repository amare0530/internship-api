import os
import pandas as pd
from sqlalchemy import create_engine
from dotenv import load_dotenv
import random
from datetime import datetime

# --- 0. ç’°å¢ƒè¨­å®š ---
load_dotenv() 

DATABASE_URL = os.getenv("DATABASE_URL")
if not DATABASE_URL:
    # å¦‚æœ DATABASE_URL æœªè¨­å®šï¼Œç¨‹å¼æœƒåœ¨æ­¤è™•åœæ­¢
    raise ValueError("DATABASE_URL ç’°å¢ƒè®Šæ•¸æœªè¨­å®šï¼è«‹æª¢æŸ¥æ‚¨çš„ .env æª”æ¡ˆã€‚")

TARGET_ROWS = 200000
BASE_DATA_PATH = "dataAPPsource.csv" # ä½¿ç”¨æ‚¨ä¸Šå‚³çš„æª”æ¡ˆåç¨±

# --- 1. è³‡æ–™æ“´å……åŠŸèƒ½ (å·²ä¿®æ­£ TypeError) ---

def augment_data_from_csv(base_data_path: str, target_rows: int):
    """
    è®€å–æœ¬åœ° CSV æ•¸æ“šï¼Œä¸¦å°‡å…¶æ“´å……åˆ°ç›®æ¨™ç­†æ•¸ã€‚
    """
    print(f"--- è®€å–åŸºç¤ CSV æª”æ¡ˆï¼š{base_data_path} ---")
    
    try:
        base_df = pd.read_csv(base_data_path, encoding='utf-8')
    except FileNotFoundError:
        raise FileNotFoundError(f"éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°åŸºç¤ CSV æª”æ¡ˆ {base_data_path}ï¼Œè«‹ç¢ºèªå®ƒåœ¨å°ˆæ¡ˆæ ¹ç›®éŒ„ã€‚")
    except Exception as e:
        print(f"è®€å– CSV ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
        return None

    initial_count = len(base_df)
    print(f"âœ… æˆåŠŸè®€å– {initial_count} ç­†åŸºç¤æ•¸æ“šã€‚")
    
    # ------------------ ğŸš¨ ä¿®æ­£ Type Error å€åŸŸ ğŸš¨ ------------------
    # ç¢ºä¿è–ªè³‡æ¬„ä½æ˜¯æ•¸å­—ã€‚errors='coerce' æœƒå°‡éæ•¸å­—å€¼ï¼ˆå¦‚'é¢è­°'ï¼‰è½‰æ›ç‚º NaNã€‚
    # .fillna(0) å°‡ NaN è¨­ç‚º 0ã€‚astype(int) è½‰æ›ç‚ºæ•´æ•¸ã€‚
    base_df['NT_L'] = pd.to_numeric(base_df['NT_L'], errors='coerce').fillna(0).astype(int)
    base_df['NT_U'] = pd.to_numeric(base_df['NT_U'], errors='coerce').fillna(0).astype(int)
    # ----------------------------------------------------------------

    if initial_count >= target_rows:
        return base_df.head(target_rows)

    # è¨ˆç®—éœ€è¦è¤‡è£½çš„æ¬¡æ•¸
    replication_factor = (target_rows // initial_count) + 1
    
    # æ“´å……æ•¸æ“šçš„éš¨æ©Ÿè®Šæ•¸
    common_cities = ['å°åŒ—å¸‚', 'æ–°åŒ—å¸‚', 'æ¡ƒåœ’å¸‚', 'å°ä¸­å¸‚', 'é«˜é›„å¸‚', 'å°å—å¸‚', 'æ–°ç«¹å¸‚']
    experience_levels = ['ç„¡', '1å¹´ä»¥ä¸Š', '2å¹´ä»¥ä¸Š', '3å¹´ä»¥ä¸Š', '5å¹´ä»¥ä¸Š']
    salary_adjustments = [-1000, 0, 1000, 2000, 3000, 5000, 10000]

    augmented_list = []
    
    for i in range(replication_factor):
        temp_df = base_df.copy()
        
        # éš¨æ©Ÿèª¿æ•´è–ªè³‡ (ç¾åœ¨ NT_L å·²ç¶“æ˜¯æ•¸å­—äº†ï¼Œå¯ä»¥é€²è¡ŒåŠ æ³•é‹ç®—)
        adj = random.choice(salary_adjustments)
        temp_df['NT_L'] = temp_df['NT_L'] + adj
        temp_df['NT_U'] = temp_df['NT_U'] + adj
        temp_df['NT_L'] = temp_df['NT_L'].apply(lambda x: max(0, x))
        
        # éš¨æ©Ÿèª¿æ•´åœ°é»
        temp_df['CITYNAME'] = [random.choice(common_cities) for _ in range(len(temp_df))]
        
        # éš¨æ©Ÿèª¿æ•´ç¶“é©—
        temp_df['EXPERIENCE'] = [random.choice(experience_levels) for _ in range(len(temp_df))]
        
        # è¼•å¾®ä¿®æ”¹å…¬å¸åç¨± (ç¢ºä¿æ•¸æ“šçœ‹èµ·ä¾†ä¸ä¸€æ¨£)
        temp_df['COMPNAME'] = temp_df['COMPNAME'].astype(str) + " (" + str(i % 5) + ")"
        
        augmented_list.append(temp_df)
        
    final_df = pd.concat(augmented_list, ignore_index=True)
    
    print(f"âœ… æ•¸æ“šæ“´å……å®Œæˆï¼Œç¸½ç­†æ•¸ï¼š{len(final_df)} ç­†ã€‚")
    return final_df.head(target_rows)

# --- 2. è³‡æ–™åº«åŒ¯å…¥åŠŸèƒ½ (å·²ä¿®å¾© NameError èˆ‡å„ªåŒ–åˆ†æ‰¹å¯«å…¥) ---

def import_data_to_db(df: pd.DataFrame, table_name="jobs"):
    """
    å°‡ DataFrame æ•¸æ“šåŒ¯å…¥ PostgreSQL è³‡æ–™åº«ï¼Œä½¿ç”¨ chunksize åˆ†æ‰¹å¯«å…¥ã€‚
    """
    if df is None or df.empty:
        print("æ²’æœ‰æ•¸æ“šå¯åŒ¯å…¥ï¼Œæ“ä½œçµ‚æ­¢ã€‚")
        return
    
    # ----------------------------------------------------
    # é€™æ˜¯å…ˆå‰éºå¤±çš„æ¬„ä½æ¸…ç†é‚è¼¯ï¼Œå¿…é ˆåœ¨é€™è£¡åŸ·è¡Œï¼
    # ----------------------------------------------------
    
    # æ•¸æ“šæ¬„ä½å°æ‡‰
    df_clean = df.rename(columns={
        'OCCU_DESC': 'title',       
        'JOB_DETAIL': 'detail',     
        'COMPNAME': 'company',      
        'NT_L': 'salary_min',       
        'NT_U': 'salary_max',       
        'CITYNAME': 'location',     
        'EXPERIENCE': 'experience', 
    })
    
    # å¢åŠ  AI ç›¸é—œçš„æ¨¡æ“¬æ¬„ä½
    df_clean['match_score'] = [random.uniform(0.5, 0.95) for _ in range(len(df_clean))]
    df_clean['ai_risk_level'] = [random.choice(['Low', 'Medium', 'High']) for _ in range(len(df_clean))]
    
    # æœ€çµ‚åªä¿ç•™éœ€è¦çš„æ¬„ä½
    cols_to_keep = ['title', 'detail', 'company', 'salary_min', 'salary_max', 'location', 'experience', 'match_score', 'ai_risk_level']
    df_to_import = df_clean.filter(items=cols_to_keep)
    
    # ----------------------------------------------------

    print(f"--- é–‹å§‹åŒ¯å…¥ {len(df_to_import)} ç­†æ•¸æ“šåˆ°è³‡æ–™åº«ï¼Œåˆ†æ‰¹å¯«å…¥ä¸­... ---")
    
    try:
        # ç”±æ–¼æ‚¨ä¹‹å‰é‡åˆ°äº¤æ˜“é–å®šå•é¡Œï¼Œé€™è£¡å†æ¬¡å»ºç«‹é€£ç·šå¼•æ“
        engine = create_engine(DATABASE_URL)
        
        # é—œéµå„ªåŒ–ï¼šä½¿ç”¨ chunksize=5000 é€²è¡Œåˆ†æ‰¹å¯«å…¥ï¼Œé™ä½è³‡æ–™åº«å£“åŠ›
        df_to_import.to_sql(
            table_name, 
            engine, 
            if_exists='replace', 
            index=False, 
            method='multi',
            chunksize=5000  # <--- åˆ†æ‰¹å¯«å…¥
        )
        print(f"âœ… æ•¸æ“šæˆåŠŸåŒ¯å…¥è¡¨æ ¼ '{table_name}'ï¼ç¸½ç­†æ•¸ï¼š{len(df_to_import)}")
        
    except Exception as e:
        print(f"âŒ è³‡æ–™åº«åŒ¯å…¥å¤±æ•—ï¼éŒ¯èª¤ï¼š{e}")
# --- 3. ä¸»åŸ·è¡Œå€å¡Š ---

if __name__ == "__main__":
    final_data_df = augment_data_from_csv(BASE_DATA_PATH, TARGET_ROWS)
    
    if final_data_df is not None:
        import_data_to_db(final_data_df, table_name="jobs")